# ğŸ§  Adobe HackStreet Boys - Dâ”‚       â””â”€â”€ ... (your PDF files)
â”‚
â”œâ”€â”€ ğŸ³ Dockerfile                     # Multi-stage container build
â”œâ”€â”€ ğŸ³ docker-compose.yml             # Container orchestration
â”œâ”€â”€ ğŸ“‹ .dockerignore                  # Build context exclusions Pipeline

Extract and prioritize relevant document sections for specific personas and their job-to-be-done.

## ğŸ¯ Project Overview

This is a **CPU-friendly document intelligence pipeline** that processes PDF documents and extracts the most relevant sections using semantic similarity. The system analyzes documents based on a specific persona (e.g., "Travel Planner") and their job-to-be-done (e.g., "Plan a 4-day trip for college friends").

## ğŸ“ Project Structure

```
HackStreet-Boys-Adobe-1BAni/
â”œâ”€â”€ ğŸ“‚ core/                          # Main processing engine
â”‚   â”œâ”€â”€ __init__.py                   # Module initialization
â”‚   â”œâ”€â”€ embedder.py                   # Semantic embedding & similarity
â”‚   â”œâ”€â”€ sectioner_pymupdf.py          # PDF text extraction
â”‚   â”œâ”€â”€ schemas.py                    # Data models (Pydantic)
â”‚   â”œâ”€â”€ format.py                     # Single-threaded processing
â”‚   â”œâ”€â”€ format_mp.py                  # Multi-threaded processing
â”‚   â”œâ”€â”€ process_collections.py        # Sequential collection processing
â”‚   â”œâ”€â”€ process_collections_mp.py     # Parallel collection processing
â”‚   â”œâ”€â”€ generate_output.py            # Output formatting & ranking
â”‚   â””â”€â”€ requirements.txt              # Python dependencies
â”‚
â”œâ”€â”€ ğŸ“‚ Collection 1/                  # Example collection
â”‚   â”œâ”€â”€ challenge1b_input.json       # Input configuration
â”‚   â”œâ”€â”€ challenge1b_output.json      # Generated results (after processing)
â”‚   â””â”€â”€ PDFs/                        # Source documents
â”‚       â”œâ”€â”€ document1.pdf
â”‚       â”œâ”€â”€ document2.pdf
â”‚       â””â”€â”€ ... (your PDF files)
â”‚
â”œâ”€â”€ ï¿½ Collection 2/                  # Sample dataset 2 (HR Forms)
â”‚   â”œâ”€â”€ challenge1b_input.json       # Input configuration
â”‚   â”œâ”€â”€ challenge1b_output.json      # Generated results
â”‚   â””â”€â”€ PDFs/                        # Source documents
â”‚       â”œâ”€â”€ Learn Acrobat - Fill and Sign.pdf
â”‚       â””â”€â”€ ... (15 Acrobat PDFs)
â”‚
â”œâ”€â”€ ğŸ“‚ Collection 3/                  # Sample dataset 3 (Menu Planning)
â”‚   â”œâ”€â”€ challenge1b_input.json       # Input configuration
â”‚   â”œâ”€â”€ challenge1b_output.json      # Generated results
â”‚   â””â”€â”€ PDFs/                        # Source documents
â”‚       â”œâ”€â”€ Breakfast Ideas.pdf
â”‚       â”œâ”€â”€ Dinner Ideas - Mains_1.pdf
â”‚       â””â”€â”€ ... (9 food PDFs)
â”‚
â”œâ”€â”€ ğŸ³ Dockerfile                     # Multi-stage container build
â”œâ”€â”€ ğŸ³ docker-compose.yml             # Development orchestration
â”œâ”€â”€ ğŸ“‹ .dockerignore                  # Build context exclusions
â”œâ”€â”€ ğŸ“– README.md                      # This file
â””â”€â”€ ğŸš« .gitignore                     # Git exclusions
```

## ğŸ³ Docker Commands & Usage

### **Build Commands**

```bash
# Standard build (for competition submission)
docker build --platform linux/amd64 -t mysolutionname:somerandomidentifier .
```

### **Run Commands**

```bash
# Competition format (as required)
docker run --rm \
  -v $(pwd):/app/collections \
  --network none \
  mysolutionname:somerandomidentifier
```

## âš¡ Features & Capabilities

- **ï¿½ï¸ CPU-only processing** - No GPU required, optimized for standard hardware
- **ğŸ“¦ Lightweight models** - Uses `all-MiniLM-L6-v2` (~90MB) under 200MB limit
- **âš¡ Fast processing** - ~30-60 seconds for full collections
- **ï¿½ Parallel processing** - Multi-collection and multi-document processing
- **ğŸ“‹ Structured output** - JSON format with Pydantic validation
- **ğŸ³ Containerized** - Docker multi-stage build for production deployment
- **ğŸ”’ Secure** - Runs as non-root user, offline-capable

## ğŸ”§ How It Works

### **Processing Pipeline**

1. **ï¿½ PDF Loading**: Extract text and detect sections using PyMuPDF
2. **âœ‚ï¸ Sentence Splitting**: Break content into individual sentences
3. **ğŸ§  Semantic Embedding**: Generate vector representations using transformers
4. **ï¿½ Similarity Scoring**: Compare against persona + job-to-be-done query
5. **ğŸ† Ranking & Selection**: Pick top 5 most relevant sections
6. **ğŸ“‹ Output Generation**: Format as structured JSON

### **Input Format** (`challenge1b_input.json`)

```json
{
  "persona": {
    "role": "Travel Planner"
  },
  "job_to_be_done": {
    "task": "Plan a trip of 4 days for a group of 10 college friends."
  },
  "documents": [
    {"filename": "South of France - Cities.pdf", "title": "Cities Guide"},
    {"filename": "South of France - Cuisine.pdf", "title": "Food Guide"}
  ]
}
```

### **Output Format** (`challenge1b_output.json`)

```json
{
  "metadata": {
    "persona": "Travel Planner",
    "job_to_be_done": "Plan a trip of 4 days for a group of 10 college friends.",
    "processing_timestamp": "2025-07-28T22:00:00"
  },
  "extracted_sections": [
    {
      "document": "South of France - Tips.pdf",
      "section_title": "Planning, and Exploring",
      "importance_rank": 1,
      "page_number": 0
    }
  ],
  "subsection_analysis": [
    {
      "document": "South of France - Tips.pdf", 
      "refined_text": "Planning a trip requires thoughtful preparation...",
      "page_number": 0
    }
  ]
}
```

## ğŸš€ Quick Start Guide

```bash
# 1. Build the image
docker build --platform linux/amd64 -t mysolutionname:somerandomidentifier .

# 2. Run with your data
docker run --rm \
  -v "$(pwd):/app/collections" \
  --network none \
  mysolutionname:somerandomidentifier

# 3. Check results in Collection*/
ls Collection*/
```

## ğŸ—ï¸ Technical Architecture

### **Core Components**

| Module | Purpose | Key Functions |
|--------|---------|---------------|
| `embedder.py` | Semantic similarity | `check_sentences_for_persona_job()` |
| `sectioner_pymupdf.py` | PDF extraction | `extract_sections_from_pdf()` |
| `generate_output.py` | Ranking & output | `get_top_5_sections()` |
| `process_collections_mp.py` | Multi-processing | `main()` (entry point) |
| `schemas.py` | Data validation | Pydantic models |

### **Models Used**

- **Embedding**: `sentence-transformers/all-MiniLM-L6-v2` (~90MB)
- **PDF Processing**: PyMuPDF (fast, lightweight)
- **Data Validation**: Pydantic v2.9+
- **Deep Learning**: PyTorch CPU-only

### **Performance Specs**

- **Memory Usage**: ~1-2GB RAM peak
- **Processing Time**: 30-60 seconds per collection
- **Model Loading**: ~5-10 seconds initial startup
- **Concurrent Collections**: Up to CPU core count

## ï¿½ Troubleshooting

### **Common Docker Issues**

```bash
# Permission issues on Linux/Mac
docker run --rm --user $(id -u):$(id -g) ...

# Network debugging (remove --network none)
docker run --rm -v $(pwd):/app/collections mysolutionname:somerandomidentifier
```

### **Common Processing Issues**

| Issue | Solution |
|-------|----------|
| "No collections found" | Ensure directories named `Collection 1`, `Collection 2`, etc. |
| "PDF extraction failed" | Check PDFs are text-based, not image-only |
| "Model download timeout" | Run once with internet, then use `--network none` |
| "Memory error" | Reduce batch size or process collections sequentially |

## ï¿½ Performance Benchmarks

**Test Environment**: 4-core CPU, 8GB RAM, SSD storage

| Collection | Documents | Pages | Processing Time |
|------------|-----------|-------|-----------------|
| Collection 1 (Travel) | 7 PDFs | ~35 pages | 45 seconds |
| Collection 2 (Forms) | 15 PDFs | ~90 pages | 78 seconds |
| Collection 3 (Food) | 9 PDFs | ~45 pages | 52 seconds |
| **Total (Parallel)** | **31 PDFs** | **~170 pages** | **~90 seconds** |

## ğŸ† Team: HackStreet Boys

Built for the **Adobe 1B Hackathon** - delivering efficient, scalable document intelligence on CPU-only infrastructure.

### **Key Innovations**

âœ… **Multi-stage Docker builds** for optimized deployment  
âœ… **Parallel processing** for multiple document collections  
âœ… **Semantic similarity** using lightweight transformer models  
âœ… **CPU-only architecture** with no GPU dependencies  
âœ… **Offline operation** with pre-downloaded models  

---

**ğŸš€ Ready to process your documents? Try the Docker commands above!**